{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e068d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time, os\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "N_KEYWORDS= 500\n",
    "N_AUTHORS = 2302"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a819ff0-f599-485a-9a7f-fbc201c06395",
   "metadata": {},
   "source": [
    "### Read train data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f368f965-427d-455f-85a3-8c0ae23bf6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json('../data/train.json', orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00729311-dcd5-430b-99be-6cbe3a43e1d0",
   "metadata": {},
   "source": [
    "### Pre-process train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "763123cb-c13a-458a-8c02-ddb2e020d371",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(df, is_test_data= False):\n",
    "    if is_test_data:\n",
    "        pass\n",
    "    else:\n",
    "        df1 = pd.DataFrame(df.author)\n",
    "        df1.columns=['coauthor']\n",
    "        df2 = df.explode('author')\n",
    "        df3 = df1.join(df2)\n",
    "        df3['author_set'] = df3['author'].apply(lambda x: {x})\n",
    "        df3['coauthor_set'] = df3['coauthor'].apply(lambda x: set(x))\n",
    "        df3['coauthor'] = df3['coauthor_set'] - df3['author_set']\n",
    "        df3['coauthor'] = df3['coauthor'].apply(lambda x: list(x))\n",
    "        df3 = df3[['keywords', 'coauthor', 'author']]\n",
    "        mlb_keywords = MultiLabelBinarizer(sparse_output=True, classes=list(range(500)))\n",
    "        keywords_oh = mlb_keywords.fit_transform(df3.keywords).toarray()\n",
    "\n",
    "        mlb_coauthor = MultiLabelBinarizer(sparse_output=True, classes=list(range(2302)))\n",
    "        coauthor_oh = mlb_coauthor.fit_transform(df3.coauthor).toarray()\n",
    "\n",
    "        features = np.hstack((keywords_oh, coauthor_oh))\n",
    "        labels = df3.author.to_list()\n",
    "    \n",
    "    return features, labels, mlb_keywords, mlb_coauthor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adb6c377-8889-42ae-9951-c523cb7b41eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, mlb_keywords, mlb_coauthor = preprocess_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22512d21-ecff-4906-a346-14c9c44a989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([48000, 2802])\n",
      "torch.Size([48000])\n"
     ]
    }
   ],
   "source": [
    "feature_processed = torch.from_numpy(np.array(features)).float()\n",
    "labels_processed = torch.from_numpy(np.array(labels)).long()\n",
    "\n",
    "print(feature_processed.shape)\n",
    "print(labels_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf0cf80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32160 32160\n",
      "15840 15840\n"
     ]
    }
   ],
   "source": [
    "# split raw train data into train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(feature_processed, labels_processed, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "print(len(X_train), len(y_train))\n",
    "print(len(X_val), len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d789a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = []\n",
    "testset = []\n",
    "\n",
    "for i in range(len(X_train)):\n",
    "    trainset.append((X_train[i], y_train[i]))\n",
    "    \n",
    "for i in range(len(X_val)):\n",
    "    testset.append((X_val[i], y_val[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "753aee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2802])\n",
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)\n",
    "\n",
    "papers, labels = iter(train_loader).next()\n",
    "\n",
    "print(papers.size())\n",
    "print(labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83a72625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_features, n_classes):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        \n",
    "        # Register weight matrix and bias term as model parameters - automatically tracks operations for gradient computation\n",
    "        self.W = torch.nn.Parameter(torch.nn.init.xavier_uniform_(torch.empty([n_features, n_classes]))) # Weights \n",
    "        self.b = torch.nn.Parameter(torch.zeros([n_classes])) # Biases\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for logistic regression.\n",
    "        Input: Tensor x of shape []\n",
    "        Output: Logits W @ x + b\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = x.view(batch_size, -1) # Flatten data, retaining batch size\n",
    "        out = torch.matmul(x, self.W) + self.b # compute scores \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e8137b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2802, 2302])\n",
      "torch.Size([2302])\n"
     ]
    }
   ],
   "source": [
    "n_features, n_classes = N_KEYWORDS + N_AUTHORS, N_AUTHORS\n",
    "logistic_regression_model = LogisticRegressionModel(n_features, n_classes)\n",
    "\n",
    "for p in logistic_regression_model.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0665907",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, criterion, test_loader):\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    test_preds = []\n",
    "    test_labels = []\n",
    "    for i, data in enumerate(test_loader):\n",
    "        x, labels = data\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(x)    # Compute scores\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            test_loss += criterion(input=logits, target=labels).item()\n",
    "            test_preds.append(predictions)\n",
    "            test_labels.append(labels)\n",
    "            \n",
    "    test_preds = torch.cat(test_preds)\n",
    "    test_labels = torch.cat(test_labels)\n",
    "    \n",
    "    test_accuracy = torch.eq(test_preds, test_labels).float().mean().item()\n",
    "    \n",
    "    print(f'[TEST]  Mean loss {round(test_loss/len(test_loader), 4)} | Accuracy {round(test_accuracy, 4)}')\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, test_loader, optimizer, n_epochs=25):\n",
    "    \"\"\"\n",
    "    Generic training loop for supervised multiclass learning\n",
    "    \"\"\"\n",
    "    LOG_INTERVAL = 252\n",
    "    running_loss, running_accuracy = list(), list()\n",
    "    start_time = time.time()\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(n_epochs): # Loop over training dataset 'n_epochs' times\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_loader): # Loop over elements in training set\n",
    "            x, labels = data\n",
    "            logits = model(x)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            train_acc = torch.mean(torch.eq(predictions, labels).float()).item()\n",
    "            loss = criterion(input=logits, target=labels)\n",
    "            \n",
    "            loss.backward()          # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()         # Update weight parameter using SGD\n",
    "            optimizer.zero_grad()    # Reset gradients to zero for next iteration\n",
    "            \n",
    "            # ============================================================================\n",
    "            # You can safely ignore the boilerplate code below - just reports metrics over\n",
    "            # training and test sets\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "            running_accuracy.append(train_acc)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            if i % LOG_INTERVAL == 0:  # Log training stats\n",
    "                deltaT = time.time() - start_time\n",
    "                mean_loss = epoch_loss / (i+1)\n",
    "                print(f'[TRAIN] Epoch {epoch} [{i}/{len(train_loader)}]| Mean loss {round(mean_loss, 4)} | Accuracy {round(train_acc, 4)} | Time {round(deltaT, 4)} s'.format(epoch, \n",
    "                    i, len(train_loader), mean_loss, train_acc, deltaT))\n",
    "        \n",
    "        print(f'>>>>>>  Epoch {epoch} Done| Mean loss {round(epoch_loss/len(train_loader), 4)}')\n",
    "\n",
    "        test(model, criterion, test_loader)\n",
    "\n",
    "    return running_loss, running_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "36afa37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] Epoch 0 [0/252]| Mean loss 0.9662 | Train accuracy 0.9297 | Time 0.1054 s\n",
      ">>>>>>  Epoch 0 Done| Mean loss 1.0346\n",
      "[TEST]  Mean loss 5.0784 | Accuracy 0.1451\n",
      "[TRAIN] Epoch 1 [0/252]| Mean loss 0.8338 | Train accuracy 0.9375 | Time 16.767 s\n",
      ">>>>>>  Epoch 1 Done| Mean loss 0.9824\n",
      "[TEST]  Mean loss 5.0738 | Accuracy 0.1456\n",
      "[TRAIN] Epoch 2 [0/252]| Mean loss 0.9085 | Train accuracy 0.9141 | Time 33.2633 s\n",
      ">>>>>>  Epoch 2 Done| Mean loss 0.935\n",
      "[TEST]  Mean loss 5.0726 | Accuracy 0.1482\n",
      "[TRAIN] Epoch 3 [0/252]| Mean loss 0.8731 | Train accuracy 0.9688 | Time 49.8258 s\n",
      ">>>>>>  Epoch 3 Done| Mean loss 0.891\n",
      "[TEST]  Mean loss 5.0684 | Accuracy 0.1492\n",
      "[TRAIN] Epoch 4 [0/252]| Mean loss 0.868 | Train accuracy 0.9375 | Time 66.6057 s\n",
      ">>>>>>  Epoch 4 Done| Mean loss 0.849\n",
      "[TEST]  Mean loss 5.0685 | Accuracy 0.1507\n",
      "[TRAIN] Epoch 5 [0/252]| Mean loss 0.6755 | Train accuracy 0.9453 | Time 84.5124 s\n",
      ">>>>>>  Epoch 5 Done| Mean loss 0.8109\n",
      "[TEST]  Mean loss 5.0674 | Accuracy 0.1521\n",
      "[TRAIN] Epoch 6 [0/252]| Mean loss 0.7169 | Train accuracy 0.9531 | Time 104.776 s\n",
      ">>>>>>  Epoch 6 Done| Mean loss 0.7742\n",
      "[TEST]  Mean loss 5.0677 | Accuracy 0.1532\n",
      "[TRAIN] Epoch 7 [0/252]| Mean loss 0.7184 | Train accuracy 0.9375 | Time 122.328 s\n",
      ">>>>>>  Epoch 7 Done| Mean loss 0.7415\n",
      "[TEST]  Mean loss 5.068 | Accuracy 0.1544\n",
      "[TRAIN] Epoch 8 [0/252]| Mean loss 0.6923 | Train accuracy 0.9531 | Time 150.4367 s\n",
      ">>>>>>  Epoch 8 Done| Mean loss 0.7093\n",
      "[TEST]  Mean loss 5.0672 | Accuracy 0.1542\n",
      "[TRAIN] Epoch 9 [0/252]| Mean loss 0.5245 | Train accuracy 0.9844 | Time 169.1862 s\n",
      ">>>>>>  Epoch 9 Done| Mean loss 0.6803\n",
      "[TEST]  Mean loss 5.0693 | Accuracy 0.1556\n",
      "[TRAIN] Epoch 10 [0/252]| Mean loss 0.6028 | Train accuracy 0.9688 | Time 186.1824 s\n",
      ">>>>>>  Epoch 10 Done| Mean loss 0.6523\n",
      "[TEST]  Mean loss 5.0701 | Accuracy 0.1578\n",
      "[TRAIN] Epoch 11 [0/252]| Mean loss 0.5703 | Train accuracy 0.9922 | Time 204.3158 s\n",
      ">>>>>>  Epoch 11 Done| Mean loss 0.6271\n",
      "[TEST]  Mean loss 5.0706 | Accuracy 0.1579\n",
      "[TRAIN] Epoch 12 [0/252]| Mean loss 0.6207 | Train accuracy 0.9766 | Time 224.3752 s\n",
      ">>>>>>  Epoch 12 Done| Mean loss 0.6026\n",
      "[TEST]  Mean loss 5.0727 | Accuracy 0.1594\n",
      "[TRAIN] Epoch 13 [0/252]| Mean loss 0.5379 | Train accuracy 0.9531 | Time 240.9275 s\n",
      ">>>>>>  Epoch 13 Done| Mean loss 0.5801\n",
      "[TEST]  Mean loss 5.0739 | Accuracy 0.16\n",
      "[TRAIN] Epoch 14 [0/252]| Mean loss 0.6285 | Train accuracy 0.9609 | Time 259.8261 s\n",
      ">>>>>>  Epoch 14 Done| Mean loss 0.5586\n",
      "[TEST]  Mean loss 5.0763 | Accuracy 0.1606\n",
      "[TRAIN] Epoch 15 [0/252]| Mean loss 0.5548 | Train accuracy 0.9844 | Time 280.0915 s\n",
      ">>>>>>  Epoch 15 Done| Mean loss 0.538\n",
      "[TEST]  Mean loss 5.0765 | Accuracy 0.1614\n",
      "[TRAIN] Epoch 16 [0/252]| Mean loss 0.4392 | Train accuracy 0.9922 | Time 306.2163 s\n",
      ">>>>>>  Epoch 16 Done| Mean loss 0.5189\n",
      "[TEST]  Mean loss 5.079 | Accuracy 0.1617\n",
      "[TRAIN] Epoch 17 [0/252]| Mean loss 0.5855 | Train accuracy 0.9766 | Time 327.6421 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8k/k2zcnjw50452k6m819yxfjdm0000gn/T/ipykernel_83647/3441717924.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_regression_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogistic_regression_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/8k/k2zcnjw50452k6m819yxfjdm0000gn/T/ipykernel_83647/642361612.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, optimizer, n_epochs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Loop over elements in training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8k/k2zcnjw50452k6m819yxfjdm0000gn/T/ipykernel_83647/2394647154.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Flatten data, retaining batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m \u001b[0;31m# compute scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(logistic_regression_model.parameters(), lr=0.2, momentum=0.9)\n",
    "\n",
    "lr_loss, lr_acc = train(logistic_regression_model, train_loader, test_loader, optimizer, n_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ffdde58d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1532,  0.4425, -0.0121,  ..., -0.1216, -0.0204, -0.0290],\n",
      "        [-0.1447, -0.1757, -0.2751,  ...,  0.1442, -0.2992,  0.3469],\n",
      "        [-0.1464, -0.0516, -0.0476,  ...,  0.0323, -0.0594, -0.0955],\n",
      "        ...,\n",
      "        [-0.0194,  0.0106,  0.0173,  ..., -1.0582,  0.0282,  0.0130],\n",
      "        [-0.0022,  0.0090, -0.0135,  ...,  0.0097, -0.1162, -0.0142],\n",
      "        [-0.0184,  0.0150,  0.0314,  ..., -0.0309,  0.0241, -0.5211]],\n",
      "       requires_grad=True)\n",
      "torch.Size([2802, 2302])\n",
      "Parameter containing:\n",
      "tensor([-0.5396, -0.1866,  0.3546,  ...,  0.2829,  0.2585, -0.3013],\n",
      "       requires_grad=True)\n",
      "torch.Size([2302])\n"
     ]
    }
   ],
   "source": [
    "for param in logistic_regression_model.parameters():\n",
    "    print(param)\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f70d60f-f695-4b63-a386-55377ebce0fb",
   "metadata": {},
   "source": [
    "### Transform test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e0d4c82-935f-4742-8f9f-20d5dc22c84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>venue</th>\n",
       "      <th>keywords</th>\n",
       "      <th>year</th>\n",
       "      <th>coauthor</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>[260, 6, 390, 136, 7, 11, 17, 285, 288, 162, 4...</td>\n",
       "      <td>2017</td>\n",
       "      <td>[]</td>\n",
       "      <td>988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>94</td>\n",
       "      <td>[260, 454, 137, 14, 400, 274, 339, 213, 280, 2...</td>\n",
       "      <td>2019</td>\n",
       "      <td>[1001]</td>\n",
       "      <td>2123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31</td>\n",
       "      <td>[390, 198, 7, 461, 462, 14, 404, 277, 24, 473,...</td>\n",
       "      <td>2014</td>\n",
       "      <td>[]</td>\n",
       "      <td>1578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>[195, 6, 390, 10, 459, 464, 338, 146, 276, 466...</td>\n",
       "      <td>2010</td>\n",
       "      <td>[1347]</td>\n",
       "      <td>2072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>162</td>\n",
       "      <td>[64, 1, 260, 457, 73, 147, 282, 27, 156, 43, 3...</td>\n",
       "      <td>2016</td>\n",
       "      <td>[1107]</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  venue                                           keywords  year coauthor  \\\n",
       "0        [260, 6, 390, 136, 7, 11, 17, 285, 288, 162, 4...  2017       []   \n",
       "1    94  [260, 454, 137, 14, 400, 274, 339, 213, 280, 2...  2019   [1001]   \n",
       "2    31  [390, 198, 7, 461, 462, 14, 404, 277, 24, 473,...  2014       []   \n",
       "3     6  [195, 6, 390, 10, 459, 464, 338, 146, 276, 466...  2010   [1347]   \n",
       "4   162  [64, 1, 260, 457, 73, 147, 282, 27, 156, 43, 3...  2016   [1107]   \n",
       "\n",
       "   target  \n",
       "0     988  \n",
       "1    2123  \n",
       "2    1578  \n",
       "3    2072  \n",
       "4     995  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_json('../data/test.json', orient='index')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2be2523-1750-4363-84a9-0d1cb282dcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2802)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords_oh_test = mlb_keywords.transform(test_df.keywords).toarray()\n",
    "coauthor_oh_test = mlb_coauthor.transform(test_df.coauthor).toarray()\n",
    "features_test = np.hstack((keywords_oh_test, coauthor_oh_test))\n",
    "\n",
    "features_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b1570a-efe3-4b87-8c44-d031e1f42c7d",
   "metadata": {},
   "source": [
    "### Predict Kaggle test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "283a0b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1056,  0.8017, -0.3196,  ..., -1.3770,  0.0972,  3.1753],\n",
      "        [ 1.8976,  0.1104, -0.4413,  ..., -0.2809,  1.4967,  0.4374],\n",
      "        [-2.3686, -1.3717, -0.4697,  ..., -1.3212, -1.4522, -0.4854],\n",
      "        ...,\n",
      "        [ 0.7753, -3.1361,  2.4074,  ...,  0.1092,  0.1472, -1.6790],\n",
      "        [ 1.6290, -0.9589,  1.6027,  ..., -0.7128,  2.3460,  1.6931],\n",
      "        [-0.9993, -0.0970,  0.6584,  ...,  0.3550, -0.1605, -1.2075]],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tensor_test = torch.tensor(np.array(features_test)).float()\n",
    "\n",
    "predictions = torch.matmul(tensor_test, logistic_regression_model.W) + logistic_regression_model.b\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6594b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2000, 2302])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.1056,  0.8017, -0.3196,  ..., -1.3770,  0.0972,  3.1753],\n",
       "        [ 1.8976,  0.1104, -0.4413,  ..., -0.2809,  1.4967,  0.4374],\n",
       "        [-2.3686, -1.3717, -0.4697,  ..., -1.3212, -1.4522, -0.4854],\n",
       "        ...,\n",
       "        [ 0.7753, -3.1361,  2.4074,  ...,  0.1092,  0.1472, -1.6790],\n",
       "        [ 1.6290, -0.9589,  1.6027,  ..., -0.7128,  2.3460,  1.6931],\n",
       "        [-0.9993, -0.0970,  0.6584,  ...,  0.3550, -0.1605, -1.2075]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_arr = predictions.tolist()\n",
    "\n",
    "print(predictions.shape)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a8d54ab-f74b-40ed-bac2-c402420004f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_score_list = []\n",
    "\n",
    "for index, target in test_df.target.iteritems():\n",
    "    prob = predictions_arr[index][target]\n",
    "    prob_score = 1 / (1 + np.exp(-prob))  # sigmoid function\n",
    "    prob_score_list.append(prob_score) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085b8b6-8990-4e4b-96fb-8d180e4a85e2",
   "metadata": {},
   "source": [
    "### Output result to CSV for Kaggle submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "a684352f-2ae8-4e93-a278-eaed06177642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('5_submission.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Id', 'Predicted'])\n",
    "    test_id = 0\n",
    "    for i in prob_score_list:\n",
    "        writer.writerow([test_id, i])\n",
    "        test_id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd71ad39-ce16-47a6-b36e-a8d638efb9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
