{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "from scipy import spatial\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "from preprocessing import *\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json.load(open('../data/train.json'))\n",
    "test = json.load(open('../data/test.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coop_record(dataset):\n",
    "    '''\n",
    "    This function generate and return a dictionary that records cooperation history between each authors\n",
    "    '''\n",
    "    record = pd.DataFrame(train_preprocessing(dataset)).T\n",
    "    coop_record = defaultdict(list)\n",
    "    for coauthor, author in zip(record.coauthor, record.target):\n",
    "        coop_record[author].append(coauthor)\n",
    "    coop_record = dict(coop_record)\n",
    "    for key, value in coop_record.items():\n",
    "        coop_record[key] = list(set([j for sub in coop_record[key] for j in sub]))\n",
    "\n",
    "    return coop_record\n",
    "\n",
    "coop_record = get_coop_record(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load('word2vec.bin')\n",
    "\n",
    "def compute_similarity_matrix(dataset):\n",
    "    '''\n",
    "    This function computes the cosine similarity matrix between each keywords vector\n",
    "    and returns a similarity matrix\n",
    "    '''\n",
    "    \n",
    "    all_cur_sum = []\n",
    "    for label, record in dataset.items():\n",
    "        cur_sum = torch.zeros(128)\n",
    "        for k in record['keywords']:\n",
    "            cur_sum += torch.tensor(w2v.wv.get_vector(k))\n",
    "        cur_sum = cur_sum/len(record['keywords'])\n",
    "        cur_sum = list(cur_sum.numpy())\n",
    "        all_cur_sum.append(cur_sum)\n",
    "    \n",
    "    all_cur_sum = np.array(all_cur_sum)\n",
    "    simi_matrix = 1 - pairwise_distances(all_cur_sum, metric=\"cosine\")\n",
    "    return simi_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_feature(json_record, author):\n",
    "    '''\n",
    "    This function extracts feature from the JSON file\n",
    "\n",
    "    RETURN VALUE:\n",
    "\n",
    "    A dictionary contains all the attributes value of an instances\n",
    "    '''\n",
    "\n",
    "    venue = json_record['venue']\n",
    "    # replace any empty venue id with an arbitrary id\n",
    "    if venue == '':\n",
    "        venue = 470\n",
    "\n",
    "    return {\n",
    "        'venue':venue, \n",
    "        'keywords':json_record['keywords'], \n",
    "        'year':json_record['year'], \n",
    "        'author':author}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE, NEGATIVE = 1, 0\n",
    "simi_matrix = compute_similarity_matrix(train)\n",
    "\n",
    "def negative_sampling(index):\n",
    "    negative_samples = []\n",
    "    simi_list = list(simi_matrix[int(index)])\n",
    "    lowest_20 = list(map(simi_list.index, heapq.nsmallest(30, simi_list)))\n",
    "    for i in lowest_20:\n",
    "        negative_samples.extend(train_df['author'][i])\n",
    "    return set(negative_samples)\n",
    "\n",
    "\n",
    "def create_dataset(dataset, test=False):\n",
    "    '''\n",
    "    This function creates the dataset from the JSON file \n",
    "    and performs negative sampling as well\n",
    "\n",
    "    RETURN VALUE:\n",
    "    X: Attributes\n",
    "    y: labels (POSITIVE OR NEGATIVE)\n",
    "    '''\n",
    "\n",
    "    X, y = [], []\n",
    "    count = 0\n",
    "\n",
    "    # Don't apply negative sampling when loading test set\n",
    "    if test:\n",
    "        for index, record in dataset:\n",
    "            X.append(extract_feature(record, record['target']))\n",
    "            y.append(POSITIVE)\n",
    "\n",
    "    else:\n",
    "        for index, record in dataset:\n",
    "            authors = record['author']\n",
    "            size = len(authors)\n",
    "            for author in authors:\n",
    "                X.append(extract_feature(record, author))\n",
    "                y.append(POSITIVE)\n",
    "\n",
    "            # Remove the current set of authors from the returned negative samples\n",
    "            negative_sample = negative_sampling(index) - set(authors)\n",
    "\n",
    "            # Negative sample size are one to one to positive samples\n",
    "            negative_authors = random.sample(negative_sample, size)\n",
    "            for author in negative_authors:\n",
    "                X.append(extract_feature(record, author))\n",
    "                y.append(NEGATIVE)\n",
    "            \n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(count)\n",
    "                \n",
    "    return np.stack(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset, test=False):\n",
    "        self.X, self.y = create_dataset(dataset, test)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split training set into train set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "train_data = list(train.items())\n",
    "train_size = int(TRAIN_SIZE * len(train_data))\n",
    "\n",
    "# Random shuffle dataset and split dataset into train set and validation set\n",
    "train_data = shuffle(train_data)\n",
    "train_set = train_data[:train_size]\n",
    "valid_set = train_data[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "train_ds = MyDataset(train_set)\n",
    "valid_ds = MyDataset(valid_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    venues, years, keywords, authors, y = [], [], [], [], []\n",
    "\n",
    "    for data, label in batch:\n",
    "        venues.append(data['venue'])\n",
    "        years.append(data['year'])\n",
    "        authors.append(data['author'])\n",
    "        keywords.append(torch.LongTensor(data['keywords']))\n",
    "        y.append(label)\n",
    "\n",
    "    return {\n",
    "        'venues':torch.LongTensor(venues), 'keywords':keywords, 'years':torch.LongTensor(years),\n",
    "        'authors':torch.LongTensor(authors), 'labels':torch.FloatTensor(y)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=64, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_VENUES, N_YEARS, N_KEYWORDS, N_AUTHORS = 471, 20, 500+1, 2302\n",
    "weight = torch.FloatTensor(w2v.wv.vectors)\n",
    "\n",
    "class NNEmbeddings(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size):\n",
    "        super(NNEmbeddings, self).__init__()\n",
    "\n",
    "        self.venue_embedding = nn.Embedding(N_VENUES, embedding_size)\n",
    "        self.year_embedding = nn.Embedding(N_YEARS, embedding_size)\n",
    "        self.keywords_embedding = nn.Embedding(N_KEYWORDS, embedding_size)\n",
    "        self.author_embedding = nn.Embedding(N_AUTHORS, embedding_size)\n",
    "\n",
    "        encoder_layer = TransformerEncoderLayer(d_model=128, nhead=2, batch_first=True)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layer, num_layers=1)\n",
    "\n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # Activation function\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.logits = nn.Linear(output_size, 1)\n",
    "        #self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        authors = batch['authors']\n",
    "        venues = batch['venues']\n",
    "        keywords = batch['keywords']\n",
    "        labels = batch['labels']\n",
    "\n",
    "        # Sequence has vary length, padded to the longest squence of the current batch\n",
    "        keywords = pad_sequence(keywords, batch_first=True, padding_value=500)\n",
    "        # Creates a padding mask for the current batch\n",
    "        pad_mask = ~(keywords != 500)\n",
    "        k_vec = self.keywords_embedding(keywords)\n",
    "        k_vec = self.transformer_encoder(k_vec, src_key_padding_mask=pad_mask)\n",
    "\n",
    "        v_vec = self.venue_embedding(venues)\n",
    "        a_vec = self.author_embedding(authors)\n",
    "\n",
    "        statcked_k_vec = k_vec[:,0,:]\n",
    "        \n",
    "        out = self.fc1(torch.cat((v_vec, a_vec, statcked_k_vec), dim=-1))\n",
    "        out = self.activation(out)\n",
    "        #out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.logits(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINING] epoch 0 | Batch #0 | Loss 0.6751 |\n",
      "[TRAINING] epoch 0 | Batch #1000 | Loss 0.2885 |\n",
      "[TRAINING] epoch 0 | Batch #2000 | Loss 0.4676 |\n",
      "[TRAINING] epoch 0 | Batch #3000 | Loss 0.2183 |\n",
      "[TRAINING] epoch 0 | Batch #4000 | Loss 0.2327 |\n",
      "[TRAINING] epoch 0 | Batch #5000 | Loss 0.1132 |\n",
      "[TRAINING] epoch 0 | Batch #6000 | Loss 0.2826 |\n",
      "[TRAINING] epoch 0 | Batch #7000 | Loss 0.5913 |\n",
      "[TRAINING] epoch 0 | Batch #8000 | Loss 0.2087 |\n",
      "[TRAINING] epoch 0 | Batch #9000 | Loss 0.2979 |\n",
      "[TRAINING] epoch 0 | Batch #10000 | Loss 0.1141 |\n",
      "[TRAINING] epoch 0 | Batch #11000 | Loss 0.0726 |\n",
      "[TRAINING] epoch 0 | Batch #12000 | Loss 0.2005 |\n",
      "[TRAINING] epoch 1 | Batch #0 | Loss 0.0341 |\n",
      "[TRAINING] epoch 1 | Batch #1000 | Loss 0.3030 |\n",
      "[TRAINING] epoch 1 | Batch #2000 | Loss 0.2216 |\n",
      "[TRAINING] epoch 1 | Batch #3000 | Loss 0.1832 |\n",
      "[TRAINING] epoch 1 | Batch #4000 | Loss 0.1706 |\n",
      "[TRAINING] epoch 1 | Batch #5000 | Loss 0.2628 |\n",
      "[TRAINING] epoch 1 | Batch #6000 | Loss 0.0718 |\n",
      "[TRAINING] epoch 1 | Batch #7000 | Loss 0.0809 |\n",
      "[TRAINING] epoch 1 | Batch #8000 | Loss 0.3608 |\n",
      "[TRAINING] epoch 1 | Batch #9000 | Loss 0.1477 |\n",
      "[TRAINING] epoch 1 | Batch #10000 | Loss 0.1238 |\n",
      "[TRAINING] epoch 1 | Batch #11000 | Loss 0.4830 |\n",
      "[TRAINING] epoch 1 | Batch #12000 | Loss 0.3134 |\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "OUTPUT_SIZE = 256\n",
    "INPUT_SIZE =  3 * EMBEDDING_SIZE\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_EPOCHS = 2\n",
    "LOG_INTERVAL = 1000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "model = NNEmbeddings(INPUT_SIZE, EMBEDDING_SIZE, HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "def train(model, train_loader, optimizer):\n",
    "    for i in range(NUM_EPOCHS):\n",
    "        for j, batch in enumerate(train_loader):\n",
    "            logits = model(batch)\n",
    "            loss = criterion(logits.squeeze(), batch['labels'])\n",
    "            loss.backward()       # Backward pass (compute parameter gradients)\n",
    "            optimizer.step()      # Update weight parameter using SGD\n",
    "            optimizer.zero_grad() # Reset gradients to zero for next iteration\n",
    "            \n",
    "            if j % LOG_INTERVAL == 0:\n",
    "                print(f\"[TRAINING] epoch {i} | Batch #{j} | Loss {loss:.5f} |\")\n",
    "\n",
    "train(model, train_loader, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = list(test.items())\n",
    "test_ds = MyDataset(test_set, test=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_fn(x):\n",
    "    return [(1 / (1 + np.exp(-i))) for i in x]\n",
    "\n",
    "def make_prediction(dataloader, model):\n",
    "    prediction = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            output = model(batch).squeeze().numpy()\n",
    "            prediction.append(output)\n",
    "    \n",
    "    # Stack arrays in sequence horizontally and apply sigmoid function\n",
    "    return sigmoid_fn(np.hstack(prediction))\n",
    "\n",
    "probability_score = make_prediction(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Kaggle submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submission.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Id', 'Predicted'])\n",
    "    test_id = 0\n",
    "    for i in probability_score:\n",
    "        writer.writerow([test_id, i])\n",
    "        test_id += 1"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
